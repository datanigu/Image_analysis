{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to classify images using a model architecture created in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import math\n",
    "from random import shuffle\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Activation, Input, Dense, Conv2D, Dropout, Flatten, BatchNormalization\n",
    "from keras.regularizers import l1\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TerminateOnNaN, TensorBoard\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# The code below are to allow tensorflow to work with Geforce RTX-2070 GPUs\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "# dynamically grow the memory used on the GPU\n",
    "config.gpu_options.allow_growth = True  \n",
    "# to log device placement (on which device the operation ran)                                  \n",
    "# (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "config.log_device_placement = True  \n",
    "sess = tf.Session(config=config)\n",
    "# set this TensorFlow session as the default session for Keras\n",
    "set_session(sess)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define utlity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_files(root_dir, img_types):\n",
    "    #os.walk creates 3-tuple with (dirpath, dirnames, filenames)\n",
    "    \n",
    "    # Get all the root directories, subdirectories, and files\n",
    "    full_paths = [x for x in os.walk(root_dir)] \n",
    "    imgs_temp = [os.path.join(ds,f) for ds,_,fs in full_paths for f in fs if f]   \n",
    "    \n",
    "    # Filter out so only have directories with .jpg, .tiff, .tif, .png, .jpeg\n",
    "    imgs = [j for j in imgs_temp if any (k in j for k in img_types)]\n",
    "    return imgs\n",
    "\n",
    "def get_dimensions(files):\n",
    "    # Set starting points for min and max dimensions\n",
    "    min_height, min_width = 10000, 10000\n",
    "    max_height, max_width = 0, 0\n",
    "    \n",
    "    for f in files:\n",
    "        # Read in images\n",
    "        img = cv.imread(f) # Read in images\n",
    "        h,w = img.shape[:2] # get height and width\n",
    "        \n",
    "        # Update min and max values, if necessary\n",
    "        if h < min_height:\n",
    "            min_height = h \n",
    "        if h > max_height:\n",
    "            max_height = h\n",
    "        if w < min_width:\n",
    "            min_width = w\n",
    "        if w > max_width:\n",
    "            max_width = w\n",
    "            \n",
    "    return min_height, min_width, max_height, max_width\n",
    "\n",
    "def make_labels(files):\n",
    "    # Assume input is a list of complete file paths.\n",
    "    # Count the number of unique directory names that are immediate parent of the files.\n",
    "    # Order the directory names alphabetically from a-z, and associate labels accordingly.\n",
    "    set_temp = {x.split('/')[-2] for x in files} #doing as set to get only unique values\n",
    "    list_temp = list(set_temp) #Change to list so can interate over it\n",
    "    list_new = sorted(list_temp) #Alphabetizing\n",
    "    label_dict = {list_new[x]:x for x in range(len(list_new))} #create dictionary with category:index\n",
    "    \n",
    "    return label_dict\n",
    "\n",
    "def make_train_val(files, lables):\n",
    "    train=[]\n",
    "    valid = []\n",
    "    train_prop = 0.6 #proportion of data set that will be training\n",
    "    for key in labels: #going through each key\n",
    "        temp = [f for f in files if key in f] #getting all files in a specific category (ie key)\n",
    "        train.extend(temp[:math.ceil(train_prop*len(temp))]) #training data set\n",
    "        valid.extend(temp[math.ceil(train_prop*len(temp)):]) # validation data set\n",
    "    return train, valid\n",
    "\n",
    "def get_batches(files, label_map, batch_size, resize_size, num_color_channels, augment=False, predict=False):\n",
    "    shuffle(files)\n",
    "    count = 0\n",
    "    num_files = len(files)\n",
    "    num_classes = len(label_map)\n",
    "    \n",
    "    batch_out = np.zeros((batch_size, resize_size[0], resize_size[1], num_color_channels), dtype=np.uint8)\n",
    "    labels_out = np.zeros((batch_size,num_classes)) #one-hot labeling, which is why have num_classes num of col.   \n",
    "\n",
    "    while True: # while True is to ensure when yielding that start here and not previous lines\n",
    "\n",
    "        f = files[count]\n",
    "        img = cv.imread(f)       \n",
    "\n",
    "        # Resize\n",
    "        # First resize while keeping aspect ratio\n",
    "        rows,cols = img.shape[:2] # Define in input num_color_channels in case want black and white\n",
    "        rc_ratio = rows/cols\n",
    "        if resize_size[0] > int(resize_size[1]*rc_ratio):# if resize rows > rows with given aspect ratio\n",
    "            img = cv.resize(img, (resize_size[1], int(resize_size[1]*rc_ratio)))#NB: resize dim arg are col,row\n",
    "        else:\n",
    "            img = cv.resize(img, (int(resize_size[0]/rc_ratio), resize_size[0]))\n",
    "            \n",
    "        # Second, pad to final size\n",
    "        rows,cols = img.shape[:2] #find new num rows and col of resized image\n",
    "        res = np.zeros((resize_size[0], resize_size[1], num_color_channels), dtype=np.uint8)#array of zeros\n",
    "        res[(resize_size[0]-rows)//2:(resize_size[0]-rows)//2+rows,\n",
    "            (resize_size[1]-cols)//2:(resize_size[1]-cols)//2+cols,:] = img # fill in image in middle of zeros\n",
    "                \n",
    "        # Augmentation \n",
    "        if augment:            \n",
    "            rows,cols = res.shape[:2]\n",
    "            # calculates affine rotation with random angle rotation, keeping same center and scale\n",
    "            M = cv.getRotationMatrix2D((cols/2,rows/2),np.random.uniform(0.0,360.0,1),1) \n",
    "            # applies affine rotation\n",
    "            res = cv.warpAffine(res,M,(cols,rows))\n",
    "\n",
    "        # Change to gray scale if input argument num_color_channels = 1\n",
    "        if num_color_channels == 1: \n",
    "            res = cv.cvtColor(res, cv.COLOR_BGR2GRAY)# convert from bgr to gray\n",
    "            res = res[...,None] # add extra dimension with blank values to very end, needed for keras\n",
    "            \n",
    "        batch_out[count%batch_size,...] = res # put image in position in batch, never to exceed size of batch\n",
    "        \n",
    "        for k in label_map.keys():\n",
    "            if k in f: #if a category name is found in the path to the file of the image\n",
    "                labels_out[count%batch_size,:] = np_utils.to_categorical(label_map[k],num_classes) #one hot labeling\n",
    "                break   \n",
    "                \n",
    "        count += 1\n",
    "        if count == num_files:# if gone through all files, restart the counter\n",
    "            count = 0\n",
    "        if count%batch_size == 0: #if gone through enough files to make a full batch\n",
    "            if predict: # i.e., there is no label for this batch of images, so in prediction mode\n",
    "                yield batch_out.astype(np.float)/255.\n",
    "            else: # training\n",
    "                yield batch_out.astype(np.float)/255., labels_out\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convnet classifier\n",
    "class classifier():\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 n_classes,\n",
    "                 n_conv_layers=2,\n",
    "                 n_conv_filters=[32]*2, # individually customizable\n",
    "                 kernel_size=[(3,2)]*2, # list of integers or tuples\n",
    "                 n_dense_layers=1,\n",
    "                 dense_units=[32],\n",
    "                 dropout=[0.0]*3, # individually customizable\n",
    "                 strides=[(2,1)]*2,\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 l1_reg=0.0,\n",
    "                 lr=0.001\n",
    "                ):\n",
    "\n",
    "        if len(n_conv_filters) == 1:\n",
    "            n_conv_filters = n_conv_filters*n_conv_layers\n",
    "\n",
    "        if len(kernel_size) == 1:\n",
    "            kernel_size = kernel_size*n_conv_layers\n",
    "            \n",
    "        if len(dense_units) == 1:\n",
    "            dense_units = dense_units*n_dense_layers\n",
    "\n",
    "        if len(dropout) == 1:\n",
    "            dropout = dropout*(n_conv_layers+n_dense_layers)\n",
    "\n",
    "        if len(strides) == 1:\n",
    "            strides = strides*n_conv_layers\n",
    "\n",
    "        self.input_shape=input_shape\n",
    "        self.n_classes=n_classes\n",
    "        self.n_conv_layers=n_conv_layers\n",
    "        self.n_conv_filters=n_conv_filters\n",
    "        self.kernel_size=kernel_size\n",
    "        self.n_dense_layers=n_dense_layers\n",
    "        self.dense_units=dense_units\n",
    "        self.dropout=dropout\n",
    "        self.strides=strides\n",
    "        self.activation=activation\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.l1_reg=l1_reg\n",
    "        self.lr=lr\n",
    "        self.model = self.get_model()\n",
    "\n",
    "    def get_model(self):\n",
    "        I = Input(shape=self.input_shape, name='input')\n",
    "        X = I\n",
    "        # Add Conv layers\n",
    "        for i in range(self.n_conv_layers):\n",
    "            X = Conv2D(self.n_conv_filters[i], self.kernel_size[i], strides=self.strides[i], padding='same',\n",
    "                       data_format='channels_last', kernel_initializer=self.kernel_initializer,\n",
    "                       kernel_regularizer=l1(self.l1_reg), name='conv_{}'.format(i))(X)\n",
    "            X = Activation(self.activation)(X)\n",
    "#             X = BatchNormalization()(X)\n",
    "            X = Dropout(self.dropout[i])(X)\n",
    "        \n",
    "        X = Flatten()(X)\n",
    "        # Add Dense layers\n",
    "        for i in range(self.n_dense_layers):\n",
    "            X = Dense(self.dense_units[i], kernel_initializer=self.kernel_initializer,\n",
    "                      kernel_regularizer=l1(self.l1_reg), name='dense_{}'.format(i))(X)\n",
    "            X = Activation(self.activation)(X)\n",
    "#             X = BatchNormalization()(X)\n",
    "            X = Dropout(self.dropout[i+self.n_conv_layers])(X)\n",
    "        O = Dense(self.n_classes, activation='softmax', kernel_initializer=self.kernel_initializer,\n",
    "                  kernel_regularizer=l1(self.l1_reg), name='output')(X)\n",
    "        \n",
    "        model = Model(inputs=I, outputs=O)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=self.lr), metrics=['accuracy'])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_files, val_files, label_map, epochs=100, batch_size=8, common_size=(100,100), num_color_channels=3, \n",
    "          new_model=True, save_model_name='classification_model_1.hdf5'):\n",
    "    num_batches_per_epoch = len(train_files)//batch_size\n",
    "    \n",
    "    train_batch_generator = get_batches(train_files, label_map, batch_size, common_size, num_color_channels, augment=True)\n",
    "    val_batch_generator = get_batches(val_files, label_map, batch_size, common_size, num_color_channels)\n",
    "\n",
    "    checkpt = ModelCheckpoint(save_model_name, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "    \n",
    "    if new_model: # create a new model\n",
    "        #### CHANGE THIS SECTION TO CREATE NEW CONVOLUTIONAL ARCHITECTURE ###\n",
    "        model = classifier([common_size[0], common_size[1], num_color_channels],\n",
    "                           len(label_map),\n",
    "                           n_conv_layers=18,#number of convolutional layers\n",
    "                           n_conv_filters=[64],#number of filters for each conv. layer. Can just put one number to be repeated \n",
    "                           kernel_size=[(3,3)],#kernel size for each filter. Can just put one number to be repeated\n",
    "                           n_dense_layers=2,#number dense layers\n",
    "                           dense_units=[32],#number of nodes in dense layer. One number gets repeated\n",
    "                           dropout=[0.0],#proportion of nodes left out of each layer\n",
    "                           strides=([(1,1)]*2+[(2,2)])*6,#how filter moves across image. Can change stride for each filter. First number is left to right, second is up/down\n",
    "                           activation='relu',#activation function\n",
    "                           kernel_initializer='glorot_uniform',#kernel initializer\n",
    "                           l1_reg=0.0,#l1 norm regularizer\n",
    "                           lr=0.0001).model #lr = learning rate\n",
    "    else: # continue to train a previous model\n",
    "        print('Continuing training from a previous model')\n",
    "        model = load_model('models/'+save_model_name)\n",
    "\n",
    "    model.summary()\n",
    "    model.fit_generator(train_batch_generator, steps_per_epoch=num_batches_per_epoch, epochs=epochs,\n",
    "                        verbose=1, callbacks=[checkpt, TerminateOnNaN()], \n",
    "                        validation_data=val_batch_generator, validation_steps=len(val_files)//batch_size)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(files, label_map, common_size=(100,100), num_color_channels=3, saved_model_name='classification_model_1.hdf5'):\n",
    "    model = load_model(saved_model_name)\n",
    "    num_batches_per_epoch = len(files)    \n",
    "    predict_batch_generator = get_batches(files, {}, batch_size, common_size, num_color_channels)\n",
    "\n",
    "    predicts = []\n",
    "    p = model.predict_generator(predict_batch_generator, steps_per_epoch=num_batches_per_epoch)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3096\n",
      "['/Users/dtaniguchi/Research/Image_classification/Scripps_plankton_camera_system_images/Labeled_images/Centric_diatom/SPCP2-1429588285-167161-000-1784-1076-144-136.jpg', '/Users/dtaniguchi/Research/Image_classification/Scripps_plankton_camera_system_images/Labeled_images/Centric_diatom/SPCP2-1432008290-105571-001-1128-408-216-248.jpg', '/Users/dtaniguchi/Research/Image_classification/Scripps_plankton_camera_system_images/Labeled_images/Barnacle_nauplii/SPC2-1426227077-725186-000-0-1996-224-120.jpg', '/Users/dtaniguchi/Research/Image_classification/Scripps_plankton_camera_system_images/Labeled_images/Barnacle_nauplii/SPC2-1431461547-035631-004-1772-456-136-80.jpg']\n",
      "Over all images - minimum height: 32, minimum width: 24, maximum height: 880, maximum width:920\n",
      "{'Ascidian_larvae': 0, 'Bacteriastrum': 1, 'Barnacle_cypris': 2, 'Barnacle_nauplii': 3, 'Centric_diatom': 4, 'Ceratium': 5, 'Ceratium_falcatiforme': 6, 'Ceratiusm_two_cells': 7, 'Cheatoceros': 8, 'Ciliate': 9, 'Dinoflagellate_generic': 10, 'Dinophysis_caudata': 11, 'Dinophysis_fortii': 12, 'Gonyaulax_spinifera': 13, 'Prorocentrum': 14, 'Prorocentrum_micans': 15, 'Protoperidinium': 16, 'Pyrocystis_lunula': 17}\n",
      "2084\n",
      "1401\n"
     ]
    }
   ],
   "source": [
    "# Get full paths to all classification data\n",
    "# Data is assumed to reside under the directory \"root_dir\", and data for each class is assumed to reside in a separate subfolder\n",
    "root_dir = '/Users/dtaniguchi/Research/Image_classification/Scripps_plankton_camera_system_images/Labeled_images'\n",
    "\n",
    "img_types=['.jpg', '.tiff', '.tif', '.png', '.jpeg']\n",
    "\n",
    "files = get_image_files(root_dir, img_types)\n",
    "print(len(files))\n",
    "print(files[0:4])\n",
    "\n",
    "# Get the dimension range of the data for informational purposes\n",
    "minh,minw,maxh,maxw = get_dimensions(files)\n",
    "print('Over all images - minimum height: {}, minimum width: {}, maximum height: {}, maximum width:{}'.format(minh,minw,maxh,maxw))\n",
    "\n",
    "# Assign numerical labels to categories - the number of categories is equal to the number of subfolders\n",
    "label_map = make_labels(files)\n",
    "print(label_map)\n",
    "\n",
    "# Split the data into training and validation\n",
    "train_files, val_files = make_train_val(files, label_map)\n",
    "print(len(train_files))\n",
    "print(len(val_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "common_size = (100,100)\n",
    "num_color_channels = 3\n",
    "train_files = train_files[:len(train_files)//batch_size*batch_size]\n",
    "g = get_batches(train_files, label_map, batch_size, common_size, num_color_channels, augment=True)\n",
    "b,l = next(g)\n",
    "for i in b:\n",
    "    plt.figure()\n",
    "    plt.imshow(i[...,::-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier\n",
    "# Note: all images are resized to common_size.  Change as desired. \n",
    "# Images smaller than common_size will be enlarged using interpolation.  Images larger will be shrunk using decimation.\n",
    "batch_size = 32\n",
    "epochs = 2000\n",
    "train_files = train_files[:len(train_files)//batch_size*batch_size]\n",
    "val_files = val_files[:len(val_files)//batch_size*batch_size]\n",
    "print(len(train_files))\n",
    "print(len(val_files))\n",
    "model = train(train_files, val_files, label_map, epochs=epochs, batch_size=batch_size, common_size=(200,200), num_color_channels=3, \n",
    "              new_model=True, save_model_name='classification_model_1.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
